
## LLMs From Scratch Series

1. Andrej Karpathy - [10 Vidoes](https://youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&si=EuGApF9EXdu1_an5)
    - The spelled-out intro to neural networks and backpropagation: building micrograd
    - The spelled-out intro to language modeling: building makemore
    - Building makemore Part 2: MLP
    - Building makemore Part 3: Activations & Gradients, BatchNorm
    - Building makemore Part 4: Becoming a Backprop Ninja
    - Building makemore Part 5: Building a WaveNet
    - Let's build GPT: from scratch, in code, spelled out.
    - State of GPT
    - Let's build the GPT Tokenizer
    - Let's reproduce GPT-2 (124M)

2. StatQuest with Josh Starmer - [15 Videos](https://youtube.com/playlist?list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&si=DfkDMWz58VJgBrsD)
    - Recurrent Neural Networks (RNNs), Clearly Explained!!!
    - Long Short-Term Memory (LSTM), Clearly Explained
    - Word Embedding and Word2Vec, Clearly Explained!!!
    - Sequence-to-Sequence (seq2seq) Encoder-Decoder Neural Networks, Clearly Explained!!!
    - Attention for Neural Networks, Clearly Explained!!!
    - Transformer Neural Networks, ChatGPT's foundation, Clearly Explained!!!
    - Decoder-Only Transformers, ChatGPTs specific Transformer, Clearly Explained!!!
    - Tensors for Neural Networks, Clearly Explained!!!
    - Essential Matrix Algebra for Neural Networks, Clearly Explained!!!
    - The matrix math behind transformer neural networks, one step at a time!!!
    - The StatQuest Introduction to PyTorch
    - Introduction to Coding Neural Networks with PyTorch and Lightning
    - Long Short-Term Memory with PyTorch + Lightning
    - Word Embedding in PyTorch + Lightning
    - Coding a ChatGPT Like Transformer From Scratch in PyTorch
3. Sebastian Raschka - [5 Videos](https://youtube.com/playlist?list=PLTKMiZHVd_2Licpov-ZK24j6oUnbhiPkm&si=9oqXgWnDumkgA176)
    - Developing an LLM: Building, Training, Finetuning 
    - Understanding PyTorch Buffers
    - Finetuning Open-Source LLMs
    - Insights from Finetuning LLMs with Low-Rank Adaptation
    - Building LLMs from the Ground Up: A 3-hour Coding Workshop
4. CodeEmporium - [12 Videos](https://youtube.com/playlist?list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4&si=_kt_U8h_i2QtJyPj)
    - Self Attention in Transformer Neural Networks (with Code!)
    - Multi Head Attention in Transformer Neural Networks with Code!
    - Positional Encoding in Transformer Neural Networks Explained
    - Layer Normalization - EXPLAINED (in Transformer Neural Networks)
    - Blowing up the Transformer Encoder!
    - Transformer Encoder in 100 lines of code!
    - Blowing up Transformer Decoder architecture
    - Transformer Decoder coded from scratch
    - Sentence Tokenization in Transformer Code from scratch!
    - The complete guide to Transformer neural Networks!
    - The complete Transformer Neural Network Code in 300 lines!
    - Building a Translator with Transformers

5. Jay Alammar
    - Coming soon

6. Luis Serrano
    - Coming soon


### Modue 1 - Introduction to Generative AI

| Topic                                                     | References                                                                                                                                   |
| --------------------------------------------------------- |:-------------------------------------------------------------------------------------------------------------------------------------------- |
| Introduction to Generative AI,Importance and Applications | [Intro to Generartive AI - Google Cloud Tech‚ñ∂Ô∏è](https://www.youtube.com/watch?v=G2fqAlgmoPo&pp=ygUdaW50cm9kdWN0aW9uIHRvIGdlbmVyYXRpdmUgYWk%3D) |
| Autoencoders and Variational Autoencoders (VAEs)          | [Variational Autoencoders - ArxivInsights‚ñ∂Ô∏è](https://www.youtube.com/watch?v=9zKuYvjFFS8&t=346s&pp=ygUXdmFyaWF0aW9uYWwgYXV0b2VuY29kZXI%3D)<br> [Autoencoders Explained Easily‚ñ∂Ô∏è](https://youtu.be/SSXDkfiPs7c?si=3KD2T44sQQSMFjdG)<br>[Autoencoders - Jeremy Jordanüßæ ](https://www.jeremyjordan.me/autoencoders/)  |
|Generative Adversarial Networks (GANs)| [A Friendly Introduction to Generative Adversarial Networks (GANs) - Serrano.Academy‚ñ∂Ô∏è](https://www.youtube.com/watch?v=8L11aMN5KY8&t=1076s&pp=ygUER2Fucw%3D%3D) <br> [6 GAN Architectures You Really Should Know - neptune.aiüßæ](https://neptune.ai/blog/6-gan-architectures)|
|Autoregressive Models and RBMs|[Guide to Autoregressive Models- Turingüßæ](https://www.turing.com/kb/guide-to-autoregressive-models)<br> [Autoregressive Diffusion Models - Yannic Kilcher‚ñ∂Ô∏è](https://www.youtube.com/watch?v=2h4tRsQzipQ)<br>[Restricted Boltzmann Machines (RBM)- Serrano.Academy‚ñ∂Ô∏è](https://www.youtube.com/watch?v=Fkw0_aAtwIw)<br>|
|Text Generation and Language Modeling| [Text Generation-HuggingFace](https://huggingface.co/tasks/text-generation)üßæ|

### Module 2 - Deep Learning Based  Natural Language Processing
| Topic | References |
| ----- |:---------- |
|   Word Embedding    |  [Word Embedding and Word2Vec -StatQuest](https://www.youtube.com/watch?v=viZrOnJclY0)‚ñ∂Ô∏è<br> [Word2Vec, GloVe, FastText- CodeEmporium](https://www.youtube.com/watch?v=9S0-OC4LFNo&t=386s&pp=ygUQV29yZCBFbWJlZGRpbmdzIA%3D%3D)‚ñ∂Ô∏è<br> |
|Representation Learning|[Representation Learning Complete Guide-AIM](https://analyticsindiamag.com/a-comprehensive-guide-to-representation-learning-for-beginners/#:~:text=Representation%20learning%20is%20a%20class,them%20to%20a%20given%20activity.)üßæ|
|Sequence-to-Sequence Models,Encoder-Decoder Architectures|[Sequence-to-Sequence (seq2seq) EncoderDecoder Neural Networks - StatQuest](https://www.youtube.com/watch?v=L8HKweZIOmg&pp=ygUbU2VxdWVuY2UtdG8tU2VxdWVuY2UgTW9kZWxz)‚ñ∂Ô∏è<br>[EncoderDecoder Seq2Seq Models - Kriz Moses](https://medium.com/analytics-vidhya/encoder-decoder-seq2seq-models-clearly-explained-c34186fbf49b)üßæ|
|seq2seq with Attention|[Sequence to Sequence (seq2seq) and Attention - Lena Voita](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)üßæ<br>[Attention for Neural Networks-StatQuest](https://www.youtube.com/watch?v=PSs6nxngL6k&pp=ygUsU2VxdWVuY2UgdG8gU2VxdWVuY2UgKHNlcTJzZXEpIGFuZCBBdHRlbnRpb24%3D)‚ñ∂Ô∏è|
|Self Attention,Transformers| [Introduction to Transformers - Andrej Karpathy](https://www.youtube.com/watch?v=XfpMkf4rD6E)‚ñ∂Ô∏è<br>[Attention for Neural Networks-StatQuest](https://www.youtube.com/watch?v=PSs6nxngL6k)‚ñ∂Ô∏è<br>[Self attention-H2O.ai](https://h2o.ai/wiki/self-attention/)üßæ<br>[What are Transformer Models and how do they work?-Serrano.Academy](https://www.youtube.com/watch?v=qaWMOYf4ri8)‚ñ∂Ô∏è|
|Self-Supervised Learning |[Self-Supervised Learning: The Dark Matter of Intelligence-Yannic Kilcher](https://www.youtube.com/watch?v=Ag1bw8MfHGQ)‚ñ∂Ô∏è<br>[Self-Supervised Learning and Its Applications-neptune.aiüßæ](https://neptune.ai/blog/self-supervised-learning)|
|Advanced NLP|[Stanford CS224N: NLP with Deep Learning‚ñ∂Ô∏è](https://www.youtube.com/watch?v=rmVRLeJRkl4&list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4&pp=iAQB)<br>[Natural Language Processing: Advance Techniquesüßæ](https://medium.com/analytics-vidhya/natural-language-processing-advance-techniques-in-depth-analysis-b67bca5db432) |
